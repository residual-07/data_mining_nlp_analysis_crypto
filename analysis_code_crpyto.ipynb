{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install newspaper3k\n",
    "!pip install feedparser\n",
    "!pip install nltk\n",
    "!pip install plotly "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_news_from_feed(feed_url):\n",
    "    articles = []\n",
    "    feed = feedparser.parse(feed_url)\n",
    "    for entry in feed.entries:\n",
    "        # create a newspaper article object\n",
    "        article = newspaper.Article(entry.link)\n",
    "        # download and parse the article\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        # check if the article is about Bitcoin\n",
    "        if \"Bitcoin\" in article.text:\n",
    "            # extract relevant information\n",
    "            articles.append({\n",
    "              'title': article.title,\n",
    "              'author': article.authors,\n",
    "              'publish_date': article.publish_date,\n",
    "              'link': article.url,\n",
    "              'content': article.text\n",
    "            })\n",
    "    return articles\n",
    "\n",
    "# list of feed URLs and sources\n",
    "feed_urls = [\n",
    "    ('https://www.coindesk.com/arc/outboundfeeds/rss/'),\n",
    "    ('https://cointelegraph.com/rss'),\n",
    "    ('https://rss.app/feeds/tI7GiY7M29sh0nlt.xml'),  #coingape\n",
    "    ('https://rss.app/feeds/w2bLoXDSF0AoaQME.xml'),  #utoday\n",
    "    ('https://rss.app/feeds/pWnwnLdxVlX273wi.xml'),  #coinmarketcap\n",
    "    ('https://rss.app/feeds/GlP7JzjkGqZbR76J.xml'),  #dailycoin\n",
    "    ('https://rss.app/feeds/yxAe1V9hz3aUaDhj.xml'),  #newsbtc\n",
    "    ('https://rss.app/feeds/swkPy5nTNNcNnzMI.xml') \n",
    " ]\n",
    "\n",
    "# empty list to store all articles\n",
    "articles = []\n",
    "\n",
    "# loop through the feed URLs and scrape news from each feed\n",
    "for feed_url in feed_urls:\n",
    "    articles += scrape_news_from_feed(feed_url)\n",
    "\n",
    "# check if the articles list is empty\n",
    "if not articles:\n",
    "    print(\"No articles found.\")\n",
    "else:\n",
    "    # create a DataFrame from the articles list\n",
    "    news_df = pd.DataFrame(articles)\n",
    "    print(news_df.head())\n",
    "\n",
    "    \n",
    "'''\n",
    "Method has been change in searching of terms. Instead to look in title, term will be looked up in the text. Target value may be changed.\n",
    "However, there is consideration in code for method to be changed. Target term could be found after the exeution of the scrap function in pandas df, to filter out info\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formating in codespace doesnt display in full (try in jupyter notebook)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(news_df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverse full content display   \n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# create lemmatizer and stemmer objects\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# define the POS tags you want to keep (part of speech)\n",
    "keep_pos = ['NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS']\n",
    "\n",
    "# define a function to filter out words with unwanted POS tags and exclude punctuation marks\n",
    "def filter_words(text):\n",
    "    sentences = sent_tokenize(text.lower())\n",
    "    result = []\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        words_pos = pos_tag(words)\n",
    "        filtered_words = [word for word, pos in words_pos if pos in keep_pos and word != 'is' and word.isalpha()]\n",
    "        result.append(' '.join(filtered_words).replace('\\n\\n', ''))\n",
    "    return ' '.join(result)\n",
    "\n",
    "# apply the filter_words function to the 'content' column\n",
    "news_df['clean_content'] = news_df['content'].apply(filter_words)\n",
    "\n",
    "# verify DF\n",
    "news_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most frequent word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "\n",
    "# tokenize the text and calculate frequency distribution for each row\n",
    "news_df['freq_dist'] = news_df['clean_content'].apply(lambda x: FreqDist(nltk.tokenize.word_tokenize(x)))\n",
    "\n",
    "# get the most frequent word and its frequency for each row\n",
    "news_df['most_common'] = news_df['freq_dist'].apply(lambda x: x.most_common(1)[0])\n",
    "\n",
    "# separate the most frequent word and its frequency into separate columns\n",
    "news_df[['most_common_word', 'frequency']] = pd.DataFrame(news_df['most_common'].tolist(), index=news_df.index)\n",
    "\n",
    "# drop intermediate columns\n",
    "news_df.drop(['freq_dist', 'most_common'], axis=1, inplace=True)\n",
    "\n",
    "# print the resulting DataFrame\n",
    "print(news_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the most frequent word in descending order\n",
    "news_df.sort_values(by='frequency', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicate indices\n",
    "news_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplcates \n",
    "news_df = news_df.drop_duplicates()\n",
    "#verify\n",
    "news_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 10 frequent words\n",
    "news_df_the_most = news_df.sort_values(by = 'frequency', ascending = False)\n",
    "news_df_the_most = news_df_the_most.head(10)\n",
    "news_df_the_most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot it\n",
    "import plotly.express as px\n",
    "fig = px.bar(news_df_the_most, x=\"most_common_word\", y=\"frequency\", color=\"most_common_word\", text_auto=True)\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most frequent Bigram and TrigramÂ terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets form bigram and trigrams\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "c_vec = CountVectorizer(stop_words=stoplist, ngram_range=(2,3))\n",
    "# matrix of ngrams\n",
    "ngrams = c_vec.fit_transform(news_df['clean_content'])\n",
    "# count frequency of ngrams\n",
    "count_values = ngrams.toarray().sum(axis=0)\n",
    "# list of ngrams\n",
    "vocab = c_vec.vocabulary_\n",
    "df_ngram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n",
    "            ).rename(columns={0: 'frequency', 1:'bigram/trigram'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 20 bigram/trigram \n",
    "news_df_the_bigram_trigram = df_ngram.sort_values(by = 'frequency', ascending = False)\n",
    "news_df_the_bigram_trigram = news_df_the_bigram_trigram.head(20)\n",
    "news_df_the_bigram_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot \n",
    "import plotly.express as px\n",
    "fig = px.bar(news_df_the_bigram_trigram, x=\"bigram/trigram\", y=\"frequency\", color=\"bigram/trigram\", text_auto=True)\n",
    "fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vader Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import VSA\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#create an object\n",
    "sentiments = SentimentIntensityAnalyzer()\n",
    "#create a new column and for each of the category, rate the content. Define the word count per content\n",
    "news_df[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in news_df[\"clean_content\"]]\n",
    "news_df[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in news_df[\"clean_content\"]]\n",
    "news_df[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in news_df[\"clean_content\"]]\n",
    "news_df['Compound'] = [sentiments.polarity_scores(i)[\"compound\"] for i in news_df[\"clean_content\"]]\n",
    "news_df['Word_Count'] = news_df['clean_content'].apply(lambda s : len(s.split(' ')))\n",
    "print(news_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add labels for vader sentiment scoring, reference https://github.com/cjhutto/vaderSentiment\n",
    "\n",
    "score = news_df[\"Compound\"].values\n",
    "sentiment = []\n",
    "for i in score:\n",
    "    if i >= 0.05 :\n",
    "        sentiment.append('Positive')\n",
    "    elif i <= -0.05 :\n",
    "        sentiment.append('Negative')\n",
    "    else:\n",
    "        sentiment.append('Neutral')\n",
    "news_df[\"Sentiment\"] = sentiment\n",
    "\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count function\n",
    "import plotly.express as px\n",
    "sentiment_counts = pd.DataFrame(news_df['Sentiment'].value_counts()).reset_index()\n",
    "sentiment_counts.columns = ['Sentiment', 'Count']\n",
    "\n",
    "# Create the bar chart using Plotly Express\n",
    "fig = px.bar(sentiment_counts, x='Sentiment', y='Count', color='Sentiment', text='Count')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
